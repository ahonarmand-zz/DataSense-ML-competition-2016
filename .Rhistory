# featuresToRemove = c("nocs")
# reducedTrain = dat.train[,-which(names(dat.train) %in% featuresToRemove)  ]
# reducedVal = dat.val[,-which(names(dat.val) %in% featuresToRemove ) ]
# reducedFinVal = dat.finval[,-which(names(dat.finval) %in% featuresToRemove ) ]
reducedTrain = dat.train
reducedVal = dat.val
reducedFinVal = dat.finval
dat.train.X  = sparse.model.matrix(~., reducedTrain[,-ncol(reducedTrain)])[,-1]
dat.train.y = as.logical(dat.train[,ncol(reducedTrain) ])
dat.val.X = sparse.model.matrix(~., reducedVal[,-ncol(reducedVal)])[,-1]
dat.finval.X = sparse.model.matrix(~., reducedFinVal[,-ncol(reducedFinVal)])[,-1]
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.07,
max_depth = 9,
nround=200,
subsample = 0.5,
alpha = 0.03,
colsample_bytree = 0.5,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.07,
max_depth = 9,
nround=200,
subsample = 0.5,
alpha = 0.03,
colsample_bytree = 0.4,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.07,
max_depth = 9,
nround=170,
subsample = 0.5,
alpha = 0.03,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.07,
max_depth = 9,
nround=170,
subsample = 0.6,
alpha = 0.03,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.07,
max_depth = 9,
nround=170,
subsample = 0.9,
alpha = 0.03,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.07,
max_depth = 9,
nround=170,
subsample = 0.5,
alpha = 0.04,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.07,
max_depth = 9,
nround=170,
subsample = 0.5,
alpha = 0.05,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.07,
max_depth = 9,
nround=170,
subsample = 0.5,
alpha = 0.08,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.05,
max_depth = 9,
nround=170,
subsample = 0.5,
alpha = 0.08,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.03,
max_depth = 9,
nround=170,
subsample = 0.5,
alpha = 0.08,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.01,
max_depth = 9,
nround=170,
subsample = 0.5,
alpha = 0.08,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.01,
max_depth = 8,
nround=130,
subsample = 0.5,
alpha = 0.08,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.01,
max_depth = 8,
nround=130,
subsample = 0.5,
alpha = 0.08,
lambda = 0.05,
colsample_bytree = 0.35,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
yhat.train = predict(xgb , dat.train.X)
yhat.train = yhat.train>0.5
sum(yhat.train == dat.train$morethan60kyr) / nrow(dat.train)
#clearing the memory
rm(list=ls())
#gtetting rid of the plots
dev.off()
plot.new()
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
#explanatory variables must be only categorical. Last col is response
calculateFrequencies <- function(dat){
n = nrow(dat)
d = ncol(dat)
print(d)
frequencies = list()
for (i in 1:d){
numLevels = length(levels(dat[,i]))
frequencies[[i]] = rep(0, numLevels)
for (j in 1:numLevels ) {
posInd = dat[,i] == levels(dat[,i])[j]
frequencies[[i]][j] = sum(posInd)/n
}
}
return (frequencies)
}
calculateResponseRate <- function(dat){
n = nrow(dat)
d = ncol(dat)-1
print(d)
responseInd = d+1
dat[1:2,responseInd]
responseRate = list()
for (i in 1:d){
numLevels = length(levels(dat[,i]))
responseRate[[i]] = rep(0, numLevels)
for (j in 1:numLevels ) {
posInd = dat[,i] == levels(dat[,i])[j]
prob = sum(as.logical(dat[,responseInd][posInd ]) )/sum(posInd)
responseRate[[i]][j] = prob
}
}
return (responseRate)
}
dat = read.csv ('datasense_training.csv')
dat.test = read.csv('datasense_test.csv')
CASEID = dat.test[,1]
dat.test = dat.test[,-1]
dat = dat[,-1]
n = nrow(dat)
d = ncol(dat)
nn = levels(dat$pr)
nn[(nn=="NovaScotia") | (nn=="PrinceEdwardIsland")|(nn=="NewBrunswick") | (nn=="Newfoundland_Labrador")] = "Eastern"
nn[(nn=="Saskatchewan") | (nn == "Manitoba")] = "mid"
levels(dat$pr) = nn
levels(dat.test$pr) = nn
nn = levels(dat$nochrd)
nn[(nn=="Other_manual_workers") | (nn=="Other_sales_and_service_personnel")] = "unskilled1"
nn[(nn=="Clerical_personnel") | (nn=="Semi_skilled_manual_workers")] = "unskilled2"
nn[(nn=="Intermediate_sales_and_service_personnel") | (nn == "Semi_professionals_and_technicians")] = "unskilled3"
levels(dat$nochrd) = nn
levels(dat.test$nochrd) = nn
uniProf =( (dat$nocs == "Teachers_and_professors" ) & (dat$hdgree == 13) )
uniProf = as.factor(as.numeric(uniProf))
dat = cbind( uniProf , dat)
uniProf =( (dat.test$nocs == "Teachers_and_professors" ) & (dat.test$hdgree == 13) )
uniProf = as.factor(as.numeric(uniProf))
dat.test = cbind( uniProf , dat.test)
singleAtHome =(  (dat$hhsize > 1) &(dat$marst=="Single")  )
singleAtHome = as.factor(as.numeric(singleAtHome))
dat = cbind( singleAtHome , dat)
singleAtHome =(  (dat.test$hhsize > 1) &(dat.test$marst=="Single")  )
singleAtHome = as.factor(as.numeric(singleAtHome))
dat.test = cbind( singleAtHome , dat.test)
nn = levels(dat$powst)
nn[(nn=="Worked_outside_Canada") | (nn=="Worked_in_a_different_province") ] = "Far"
levels(dat$powst) = nn
levels(dat.test$powst) = nn
dat$morethan60kyr = as.factor(dat$morethan60kyr)
#dat$hdgree = as.factor(dat$hdgree)
# dat$hrswrk = scale(dat$hrswrk)
# dat$agegrp = scale( log(dat$agegrp) )
# dat$hhsize = scale(dat$hhsize)
# featuresToRemove = c("nocs")
# dat = dat[,-which(names(dat) %in% featuresToRemove)  ]
indexOfCategorical = which(lapply(dat, class) == 'factor')
d.cat = length(indexOfCategorical)
indexOfNumeric = c(which(lapply(dat, class) == 'integer') , which(lapply(dat, class)=='numeric') )
indexOfResponse = d
set.seed(1112)
ii = ((1:n) %%25)+1
ii = sample(ii)
dat.train = dat[ (ii!=1)&(ii!=2)&(ii!=3) ,  ]
dat.val = dat[ (ii==2) | (ii==3), ]
dat.finval = dat[ii == 1 , ]
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(car)
library(Matrix)
# featuresToRemove = c("nocs")
# reducedTrain = dat.train[,-which(names(dat.train) %in% featuresToRemove)  ]
# reducedVal = dat.val[,-which(names(dat.val) %in% featuresToRemove ) ]
# reducedFinVal = dat.finval[,-which(names(dat.finval) %in% featuresToRemove ) ]
reducedTrain = dat.train
reducedVal = dat.val
reducedFinVal = dat.finval
dat.train.X  = sparse.model.matrix(~., reducedTrain[,-ncol(reducedTrain)])[,-1]
dat.train.y = as.logical(dat.train[,ncol(reducedTrain) ])
dat.val.X = sparse.model.matrix(~., reducedVal[,-ncol(reducedVal)])[,-1]
dat.finval.X = sparse.model.matrix(~., reducedFinVal[,-ncol(reducedFinVal)])[,-1]
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.07,
max_depth = 9,
nround=200,
subsample = 0.5,
alpha = 0.03,
colsample_bytree = 0.5,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
yhat.train = predict(xgb , dat.train.X)
yhat.train = yhat.train>0.5
sum(yhat.train == dat.train$morethan60kyr) / nrow(dat.train)
library(randomForest)
forest  = randomForest(morethan60kyr~. , data = dat.train ,
method = 'class', ntree = 801 , nodesize=20)
yhatForest = predict(forest, newdata = dat.val)
yhatForest = as.logical(yhatForest)
sum(yhatForest == dat.val$morethan60kyr)/length(dat.val$morethan60kyr)
library(e1071)
datNB = dat.train[,-which(names(dat.train) %in% c("mfs" , "nocs" , "pr","fol"))]
NBmodel = naiveBayes( as.factor(morethan60kyr)~. , data= datNB )
yhatNB = predict(NBmodel , newdata = dat.val  )
sum(yhatNB == dat.val$morethan60kyr)/length(dat.val$morethan60kyr)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.06,
max_depth = 9,
nround=200,
subsample = 0.5,
alpha = 0.05,
colsample_bytree = 0.5,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
yhat.train = predict(xgb , dat.train.X)
yhat.train = yhat.train>0.5
sum(yhat.train == dat.train$morethan60kyr) / nrow(dat.train)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.01,
max_depth = 9,
nround=200,
subsample = 0.5,
alpha = 0.05,
colsample_bytree = 0.5,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
yhat.train = predict(xgb , dat.train.X)
yhat.train = yhat.train>0.5
sum(yhat.train == dat.train$morethan60kyr) / nrow(dat.train)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.01,
max_depth = 9,
nround=300,
subsample = 0.5,
alpha = 0.05,
colsample_bytree = 0.5,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
yhat.train = predict(xgb , dat.train.X)
yhat.train = yhat.train>0.5
sum(yhat.train == dat.train$morethan60kyr) / nrow(dat.train)
xgb <- xgboost(data = dat.train.X,
label = dat.train.y,
verbose = 1,
eta = 0.01,
max_depth = 8,
nround=300,
subsample = 0.5,
alpha = 0.05,
colsample_bytree = 0.4,
seed = 1,
objective = "binary:logistic",
eval_metric = "error",
scale_pos_weight = sum(dat.train$morethan60kyr == FALSE)/sum(dat.train$morethan60kyr == TRUE)
)
yhat.xg = predict(xgb , dat.val.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.val$morethan60kyr) / nrow(dat.val)
yhat.xg = predict(xgb , dat.finval.X)
yhat.xg = yhat.xg>0.5
sum(yhat.xg == dat.finval$morethan60kyr) / nrow(dat.finval)
yhat.train = predict(xgb , dat.train.X)
yhat.train = yhat.train>0.5
sum(yhat.train == dat.train$morethan60kyr) / nrow(dat.train)
